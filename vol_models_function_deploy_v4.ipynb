{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Volatility Modeling\n",
    "## Master's Thesis - Empirical Study \n",
    "### Universit√† della Svizzera italiana\n",
    "\n",
    "Pietro Bonazzi - pietro.bonazzi@usi.ch\n",
    "\n",
    "Volatility Models - v.4 - DEPLOY VERSION v.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras import backend \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, ElasticNetCV\n",
    "from scipy.stats import norm\n",
    "from dieboldmariano import dm_test\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_stock, mkt_covariates, lags_RV = [1,5,22], lags_RQ = [1,5,22], RQ_add = True):\n",
    "    '''\n",
    "    This function prepares the data for the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_stock : DataFrame\n",
    "        DataFrame containing the stock data.\n",
    "    mkt_covariates : DataFrame\n",
    "        DataFrame containing the market covariates.\n",
    "    lags_RV : list, optional\n",
    "        List of lags for the realized variance. The default is [1,5,22].\n",
    "    lags_RQ : list, optional    \n",
    "        List of lags for the realized quarticity. The default is [1,5,22].\n",
    "    RQ_add : bool, optional\n",
    "        Boolean to decide whether to add the realized quarticity. The default is True.\n",
    "    '''\n",
    "\n",
    "    data_stock.index = pd.to_datetime(data_stock.index)\n",
    "    data_stock['RV_'+  str(lags_RV[0])] = data_stock['RV'].rolling(window=lags_RV[0]).mean()\n",
    "    data_stock['RV_'+  str(lags_RV[1])] = data_stock['RV_'+  str(lags_RV[0])].rolling(window=lags_RV[1]).mean()\n",
    "    data_stock['RV_'+  str(lags_RV[2])] = data_stock['RV_'+  str(lags_RV[0])].rolling(window=lags_RV[2]).mean()\n",
    "\n",
    "    if RQ_add == True: \n",
    "        data_stock['RQ_'+  str(lags_RQ[0])] = data_stock['RQ'].rolling(window=lags_RQ[0]).mean()\n",
    "        data_stock['RQ_'+  str(lags_RQ[1])] = data_stock['RQ_'+  str(lags_RQ[0])].rolling(window=lags_RQ[1]).mean()\n",
    "        data_stock['RQ_'+  str(lags_RQ[2])] = data_stock['RQ_'+  str(lags_RQ[0])].rolling(window=lags_RQ[2]).mean()\n",
    "\n",
    "    mkt_covariates.index = pd.to_datetime(mkt_covariates.index)\n",
    "    mkt_covariates['CHFUSD'] = mkt_covariates['CHFUSD'].pct_change()\n",
    "    mkt_covariates['CHFEUR'] = mkt_covariates['CHFEUR'].pct_change()\n",
    "    mkt_covariates['GSWISS10'] = mkt_covariates['GSWISS10'].diff()\n",
    "    mkt_covariates['CCFASZE'] = mkt_covariates['CCFASZE'].diff()\n",
    "    mkt_covariates['SFSNTC'] = mkt_covariates['SFSNTC'].diff()\n",
    "    mkt_covariates['SSARON'] = mkt_covariates['SSARON'].diff()\n",
    "    mkt_covariates = mkt_covariates[30:-1]\n",
    "\n",
    "    data = pd.concat([data_stock, mkt_covariates], axis=1, join='inner')\n",
    "    data['Returns'] = data['Returns'].shift(-1)\n",
    "    data['RV'] = data['RV'].shift(-1)\n",
    "    data['RQ'] = data['RQ'].shift(-1)\n",
    "    data.dropna(inplace=True)\n",
    "    data = pd.DataFrame(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_selected_features(data, features):\n",
    "\n",
    "    # Select the features\n",
    "    select_features = ['Returns', 'RV'] + features\n",
    "    data = data[select_features]\n",
    "\n",
    "    # Define the train, validation and test dates\n",
    "    train_date = data.index[int(len(data)*0.6)] # 60% of the data\n",
    "    test_date = data.index[-1] # 30% of the data\n",
    "\n",
    "    # Split the data into train and test\n",
    "    df_train = data.loc[:train_date]\n",
    "    df_test = data.loc[train_date:]\n",
    "    df_test = df_test.drop(df_test.index[0])\n",
    "\n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HAR "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HAR Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def har_bmk (df_train, df_test, plot = False):\n",
    "\n",
    "    '''\n",
    "    HAR benchmark model. Default RV lags are 1, 5 and 22.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_train : pandas.DataFrame\n",
    "        Train data.\n",
    "    df_test : pandas.DataFrame\n",
    "        Test data.\n",
    "    plot : bool, optional\n",
    "\n",
    "    '''\n",
    "\n",
    "    model = LinearRegression()\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(df_train.drop(['Returns', 'RV'], axis=1), df_train['RV'])\n",
    "    prediction_bmk = model.predict(df_test.drop(['Returns', 'RV'], axis=1))\n",
    "\n",
    "    # Calculate the RMSE\n",
    "    rmse = np.sqrt(mse(df_test['RV'], prediction_bmk))\n",
    "    \n",
    "    results_bmk = pd.DataFrame({'pred_bmk': prediction_bmk, 'RV': df_test['RV']})\n",
    "    results_bmk.index = df_test.index\n",
    "\n",
    "\n",
    "    # Calculate the violation ratio\n",
    "    alpha = 0.05\n",
    "    var = pd.DataFrame({'VAR': results_bmk['pred_bmk']* norm.ppf(alpha), 'Returns': df_test['Returns']})\n",
    "    violation_ratio = (var['Returns'] < var['VAR']).sum()/len(df_test)\n",
    "\n",
    "    #DMW test (not necessary for benchmark model)\n",
    "    dmw_test = (0,0)\n",
    "\n",
    "    if plot is True:\n",
    "        plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "        fig, ax = plt.subplots(figsize=(11, 3))\n",
    "        ax.plot(results_bmk['pred_bmk'], label='prediction', color='black', linewidth=0.9)\n",
    "        ax.plot(results_bmk['RV'], label='RV', color='grey', linewidth=0.7)\n",
    "        ax.set_title('HAR_benchmark')   \n",
    "        ax.legend(loc='upper left')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.xaxis.set_major_locator(plt.MaxNLocator(6))\n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "    return rmse, violation_ratio, dmw_test, results_bmk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HARQ Regularized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def har_regularized(df_train, df_test, results_bmk, regularization = 'Lasso', plot = False):\n",
    "    '''\n",
    "    HAR regularized model. Default RV lags are 1, 5 and 22.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df_train : pandas.DataFrame\n",
    "        Train data.\n",
    "    df_test : pandas.DataFrame\n",
    "        Test data.\n",
    "    results_bmk : pandas.DataFrame\n",
    "        Results of the HAR benchmark model.\n",
    "    regularization : str, optional\n",
    "        Regularization method. The default is 'Lasso'.\n",
    "    plot : bool, optional\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # Define type of model\n",
    "    if regularization == 'Lasso':\n",
    "        model = LassoCV() # Lasso linear regression with built-in cross-validation of the alpha parameter\n",
    "    elif regularization == 'Ridge':\n",
    "        model = RidgeCV()\n",
    "    elif regularization == 'ElasticNet':\n",
    "        model = ElasticNetCV()\n",
    "    elif regularization == 'Linear':\n",
    "        model = LinearRegression()    \n",
    "\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(df_train.drop(['Returns', 'RV'], axis=1), df_train['RV'])\n",
    "    prediction = model.predict(df_test.drop(['Returns', 'RV'], axis=1))\n",
    "\n",
    "    # Calculate the RMSE\n",
    "    rmse = np.sqrt(mse(df_test['RV'], prediction))\n",
    "    \n",
    "    results = pd.DataFrame({'pred': prediction, 'RV': df_test['RV']})\n",
    "    results.index = df_test.index\n",
    "\n",
    "    # Calculate the violation ratio\n",
    "    alpha = 0.05\n",
    "    var = pd.DataFrame({'VAR': results['pred']* norm.ppf(alpha), 'Returns': df_test['Returns']})\n",
    "    violation_ratio = (var['Returns'] < var['VAR']).sum()/len(df_test)\n",
    "\n",
    "\n",
    "    # Calculate the Diebold-Mariano test\n",
    "    df_dmw_test = pd.concat([results_bmk['pred_bmk'], results['pred'], df_test['RV']], axis=1)\n",
    "    dmw_test = dm_test(df_dmw_test['RV'], df_dmw_test['pred_bmk'], df_dmw_test['pred'], one_sided=True)\n",
    "\n",
    "    if plot is True:\n",
    "        plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "        fig, ax = plt.subplots(figsize=(11, 3))\n",
    "        ax.plot(results['pred'], label='prediction', color='black', linewidth=0.9)\n",
    "        ax.plot(results['RV'], label='RV', color='grey', linewidth=0.7)\n",
    "        ax.set_title('HAR_' + regularization)   \n",
    "        ax.legend(loc='upper left')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.xaxis.set_major_locator(plt.MaxNLocator(6))\n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "    return rmse, violation_ratio, dmw_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "### LSTM Single Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prepare the data for the LSTM model \n",
    "def windowed_dataset(x_series, y_series, n_past):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range((n_past-1), len(x_series)):\n",
    "        start_idx = x_series.index[i-n_past+1]\n",
    "        end_idx = x_series.index[i]\n",
    "        a = x_series[start_idx:end_idx].values\n",
    "        dataX.append(a)\n",
    "        dataY.append(y_series[end_idx])\n",
    "    return np.array(dataX), np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_single_layer(df_train, df_test, results_bmk, neurons = 20, n_past = 22, batch_size = 32, plot = False): \n",
    "\n",
    "    '''\n",
    "    LSTM single layer model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_train : pandas.DataFrame\n",
    "        Train data.\n",
    "    df_test : pandas.DataFrame\n",
    "        Test data.\n",
    "    results_bmk : pandas.DataFrame  \n",
    "        Results of the HAR benchmark model.\n",
    "    neurons : str, optional\n",
    "        Number of neurons in the LSTM layer. The default is '20'.\n",
    "    n_past : str, optional\n",
    "        Number of past observations to use as input. The default is '22'.\n",
    "    batch_size : str, optional\n",
    "        Batch size. The default is '32'.\n",
    "    plot : bool, optional   \n",
    "\n",
    "    Inspired by: https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/\n",
    "    https://github.com/chibui191/bitcoin_volatility_forecasting\n",
    "\n",
    "    '''\n",
    "\n",
    "\n",
    "    np.random.seed(2024)\n",
    "    n_dims = df_train.shape[1]-2\n",
    "    x_train, y_train = windowed_dataset(df_train.drop(['Returns', 'RV'], axis=1), df_train['RV'], n_past)\n",
    "\n",
    "    # Define the model\n",
    "    lstm = tf.keras.models.Sequential([ \n",
    "        tf.keras.layers.InputLayer(input_shape=[n_past, n_dims]),\n",
    "\n",
    "        tf.keras.layers.LSTM(neurons),\n",
    "        \n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    def rmse(y_true, y_pred):\n",
    "        loss = backend.sqrt(backend.mean(backend.square((y_true - y_pred))))\n",
    "        return loss\n",
    "\n",
    "    lstm.compile(loss='mse', \n",
    "               optimizer=\"adam\", \n",
    "               metrics=[rmse])\n",
    "    \n",
    "    #checkpoint = ModelCheckpoint('NN_models/lstm_single_layer.h5',\n",
    "    #                            save_best_only=True,\n",
    "    #                            monitor='val_rmse')\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=100,\n",
    "                                  restore_best_weights=True,\n",
    "                                  monitor='val_rmse')\n",
    "    \n",
    "    lstm_res = lstm.fit(x_train, y_train, \n",
    "                        callbacks=early_stopping,\n",
    "                        validation_split=0.3, shuffle=True,\n",
    "                        verbose=0, batch_size=batch_size, epochs=500)\n",
    "    \n",
    "    x_test, y_test = windowed_dataset(df_test.drop(['Returns', 'RV'], axis=1), df_test['RV'], n_past)\n",
    "    x, returns = windowed_dataset(df_test.drop(['Returns', 'RV'], axis=1), df_test['Returns'], n_past)\n",
    "\n",
    "    # Predictions\n",
    "    pred = lstm.predict(x_test)\n",
    "    df_results = pd.DataFrame({'RV': df_test['RV'].iloc[n_past-1:], 'pred': pred.flatten(), 'returns': returns})\n",
    "\n",
    "    # Calculate the RMSE\n",
    "    rmse = np.sqrt(mse(df_results['RV'], df_results['pred']))\n",
    "\n",
    "    # Calculate the violation ratio\n",
    "    alpha = 0.05\n",
    "    var = pd.DataFrame({'VAR': df_results['pred']* norm.ppf(alpha), 'Returns': df_results['returns']})\n",
    "    violation_ratio = (var['Returns'] < var['VAR']).sum()/len(df_results)\n",
    "\n",
    "    # Calculate the Diebold-Mariano test\n",
    "    df_dmw_test = pd.concat([results_bmk['pred_bmk'], df_results['pred'], df_results['RV']], axis=1).dropna()\n",
    "    dmw_test = dm_test(df_dmw_test['RV'], df_dmw_test['pred_bmk'], df_dmw_test['pred'], one_sided=True)\n",
    "\n",
    "    if plot == True:\n",
    "        plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "        fig, ax = plt.subplots(figsize=(11, 3))\n",
    "        ax.plot(df_results['pred'], label='prediction', color='black', linewidth=0.9)\n",
    "        ax.plot(df_results['RV'], label='RV', color='grey', linewidth=0.7)\n",
    "        ax.set_title('LSTM_single_layer')   \n",
    "        ax.legend(loc='upper left')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.xaxis.set_major_locator(plt.MaxNLocator(6))\n",
    "        plt.show()\n",
    "\n",
    "    return rmse, violation_ratio, dmw_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM 2-layers with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_2_layers_with_dropout(df_train, df_test, results_bmk, neurons = [32,16], n_past = 22, batch_size = 32, dropout = 0.1, plot = False): \n",
    "\n",
    "    '''\n",
    "    LSTM 2 layers with dropout model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_train : pandas.DataFrame\n",
    "        Train data.\n",
    "    df_test : pandas.DataFrame\n",
    "        Test data.\n",
    "    results_bmk : pandas.DataFrame\n",
    "        Results of the HAR benchmark model.\n",
    "    neurons : list, optional\n",
    "        Number of neurons in the LSTM layers. The default is [32,16].\n",
    "    n_past : int, optional\n",
    "        Number of past observations to use as input. The default is 22.\n",
    "    batch_size : int, optional\n",
    "        Batch size. The default is 32.\n",
    "    dropout : float, optional\n",
    "        Dropout rate. The default is 0.1.\n",
    "    plot : bool, optional\n",
    "    '''\n",
    "\n",
    "    np.random.seed(2024)\n",
    "    n_dims = df_train.shape[1]-2\n",
    "    x_train, y_train = windowed_dataset(df_train.drop(['Returns', 'RV'], axis=1), df_train['RV'], n_past)\n",
    "\n",
    "    lstm = tf.keras.models.Sequential([ \n",
    "        tf.keras.layers.InputLayer(input_shape=[n_past, n_dims]),\n",
    "        tf.keras.layers.BatchNormalization(), \n",
    "\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(neurons[0], return_sequences=True)),\n",
    "        tf.keras.layers.Dropout(dropout),\n",
    "\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(neurons[1])),\n",
    "        tf.keras.layers.Dropout(dropout),\n",
    "\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    def rmse(y_true, y_pred):\n",
    "        loss = backend.sqrt(backend.mean(backend.square((y_true - y_pred))))\n",
    "        return loss\n",
    "\n",
    "    lstm.compile(loss='mse', \n",
    "               optimizer=\"adam\", \n",
    "               metrics=[rmse])\n",
    "    \n",
    "    #checkpoint = ModelCheckpoint('NN_models/lstm_2_layers_with_dropout.h5',\n",
    "    #                            save_best_only=True,\n",
    "    #                            monitor='val_rmse')\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=100,\n",
    "                                  restore_best_weights=True,\n",
    "                                  monitor='val_rmse')\n",
    "    \n",
    "    lstm_res = lstm.fit(x_train, y_train, \n",
    "                        callbacks= early_stopping,\n",
    "                        validation_split=0.3, shuffle=True,\n",
    "                        verbose=0, batch_size=batch_size, epochs=500)\n",
    "    \n",
    "    x_test, y_test = windowed_dataset(df_test.drop(['Returns', 'RV'], axis=1), df_test['RV'], n_past)\n",
    "    x, returns = windowed_dataset(df_test.drop(['Returns', 'RV'], axis=1), df_test['Returns'], n_past)\n",
    "\n",
    "    # Predictions\n",
    "    pred = lstm.predict(x_test)\n",
    "    df_results = pd.DataFrame({'RV': df_test['RV'].iloc[n_past-1:], 'pred': pred.flatten(), 'returns': returns})\n",
    "\n",
    "\n",
    "    # Calculate the RMSE\n",
    "    rmse = np.sqrt(mse(df_results['RV'], df_results['pred']))\n",
    "\n",
    "    # Calculate the violation ratio\n",
    "    alpha = 0.05\n",
    "    var = pd.DataFrame({'VAR': df_results['pred']* norm.ppf(alpha), 'Returns': df_results['returns']})\n",
    "    violation_ratio = (var['Returns'] < var['VAR']).sum()/len(df_results)\n",
    "\n",
    "    # Calculate the Diebold-Mariano test\n",
    "    df_dmw_test = pd.concat([results_bmk['pred_bmk'], df_results['pred'], df_results['RV']], axis=1).dropna()\n",
    "    dmw_test = dm_test(df_dmw_test['RV'], df_dmw_test['pred_bmk'], df_dmw_test['pred'], one_sided=True)\n",
    "\n",
    "    if plot == True:\n",
    "        plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "        fig, ax = plt.subplots(figsize=(11, 3))\n",
    "        ax.plot(df_results['pred'], label='prediction', color='black', linewidth=0.9)\n",
    "        ax.plot(df_results['RV'], label='RV', color='grey', linewidth=0.7)\n",
    "        ax.set_title('LSTM_2_layers_with_dropout') \n",
    "        ax.legend(loc='upper left')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.xaxis.set_major_locator(plt.MaxNLocator(6))\n",
    "        plt.show()\n",
    "\n",
    "    return rmse, violation_ratio, dmw_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_3_layers_with_dropout(df_train, df_test, results_bmk, neurons = [32,16,8], n_past = 22, batch_size = 32, dropout = 0.1, plot = False): \n",
    "\n",
    "    '''\n",
    "    LSTM 3 layers with dropout model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_train : pandas.DataFrame\n",
    "        Train data.\n",
    "    df_test : pandas.DataFrame\n",
    "        Test data.\n",
    "    results_bmk : pandas.DataFrame\n",
    "        Results of the HAR benchmark model.\n",
    "    neurons : list, optional    \n",
    "        Number of neurons in the LSTM layers. The default is [32,16,8].\n",
    "    n_past : int, optional  \n",
    "        Number of past observations to use as input. The default is 22. \n",
    "    batch_size : int, optional  \n",
    "        Batch size. The default is 32.  \n",
    "    dropout : float, optional\n",
    "        Dropout rate. The default is 0.1.\n",
    "    plot : bool, optional\n",
    "    '''\n",
    "\n",
    "    np.random.seed(2024)\n",
    "    n_dims = df_train.shape[1]-2\n",
    "    x_train, y_train = windowed_dataset(df_train.drop(['Returns', 'RV'], axis=1), df_train['RV'], n_past)\n",
    "\n",
    "    lstm = tf.keras.models.Sequential([ \n",
    "        tf.keras.layers.InputLayer(input_shape=[n_past, n_dims]),\n",
    "        tf.keras.layers.BatchNormalization(), \n",
    "\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(neurons[0], return_sequences=True)),\n",
    "        tf.keras.layers.Dropout(dropout),\n",
    "\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(neurons[1], return_sequences=True)),\n",
    "        tf.keras.layers.Dropout(dropout),\n",
    "\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(neurons[2])),\n",
    "        tf.keras.layers.Dropout(dropout),\n",
    "\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    def rmse(y_true, y_pred):\n",
    "        loss = backend.sqrt(backend.mean(backend.square((y_true - y_pred))))\n",
    "        return loss\n",
    "\n",
    "    lstm.compile(loss='mse', \n",
    "               optimizer=\"adam\", \n",
    "               metrics=[rmse])\n",
    "    \n",
    "    #checkpoint = ModelCheckpoint('NN_models/lstm_2_layers_with_dropout.h5',\n",
    "    #                            save_best_only=True,\n",
    "    #                            monitor='val_rmse')\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=100,\n",
    "                                  restore_best_weights=True,\n",
    "                                  monitor='val_rmse')\n",
    "    \n",
    "    lstm_res = lstm.fit(x_train, y_train, \n",
    "                        callbacks= early_stopping,\n",
    "                        validation_split=0.3, shuffle=True,\n",
    "                        verbose=0, batch_size=batch_size, epochs=500)\n",
    "    \n",
    "    x_test, y_test = windowed_dataset(df_test.drop(['Returns', 'RV'], axis=1), df_test['RV'], n_past)\n",
    "    x, returns = windowed_dataset(df_test.drop(['Returns', 'RV'], axis=1), df_test['Returns'], n_past)\n",
    "\n",
    "    # Predictions\n",
    "    pred = lstm.predict(x_test)\n",
    "    df_results = pd.DataFrame({'RV': df_test['RV'].iloc[n_past-1:], 'pred': pred.flatten(), 'returns': returns})\n",
    "\n",
    "    # Calculate the RMSE\n",
    "    rmse = np.sqrt(mse(df_results['RV'], df_results['pred']))\n",
    "\n",
    "    # Calculate the violation ratio\n",
    "    alpha = 0.05\n",
    "    var = pd.DataFrame({'VAR': df_results['pred']* norm.ppf(alpha), 'Returns': df_results['returns']})\n",
    "    violation_ratio = (var['Returns'] < var['VAR']).sum()/len(df_results)\n",
    "\n",
    "    # Calculate the Diebold-Mariano test\n",
    "    df_dmw_test = pd.concat([results_bmk['pred_bmk'], df_results['pred'], df_results['RV']], axis=1).dropna()\n",
    "    dmw_test = dm_test(df_dmw_test['RV'], df_dmw_test['pred_bmk'], df_dmw_test['pred'], one_sided=True)\n",
    "\n",
    "    if plot == True:\n",
    "        plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "        fig, ax = plt.subplots(figsize=(11, 3))\n",
    "        ax.plot(df_results['pred'], label='prediction', color='black', linewidth=0.9)\n",
    "        ax.plot(df_results['RV'], label='RV', color='grey', linewidth=0.7)\n",
    "        ax.set_title('LSTM_3_layers_with_dropout') \n",
    "        ax.legend(loc='upper left')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.xaxis.set_major_locator(plt.MaxNLocator(6))\n",
    "        plt.show()\n",
    "\n",
    "    return rmse, violation_ratio, dmw_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_3_layers_with_dropout_nobidirectional(df_train, df_test, results_bmk, neurons = [32,16,8], n_past = 22, batch_size = 32, dropout = 0.1, plot = False): \n",
    "\n",
    "    '''\n",
    "    LSTM 3 layers with dropout model without bidirectional layers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_train : pandas.DataFrame\n",
    "        Train data.\n",
    "    df_test : pandas.DataFrame\n",
    "        Test data.\n",
    "    results_bmk : pandas.DataFrame\n",
    "        Results of the HAR benchmark model.\n",
    "    neurons : list, optional    \n",
    "        Number of neurons in the LSTM layers. The default is [32,16,8].\n",
    "    n_past : int, optional  \n",
    "        Number of past observations to use as input. The default is 22. \n",
    "    batch_size : int, optional  \n",
    "        Batch size. The default is 32.  \n",
    "    dropout : float, optional\n",
    "        Dropout rate. The default is 0.1.\n",
    "    plot : bool, optional\n",
    "    '''\n",
    "\n",
    "    np.random.seed(2024)\n",
    "    n_dims = df_train.shape[1]-2\n",
    "    x_train, y_train = windowed_dataset(df_train.drop(['Returns', 'RV'], axis=1), df_train['RV'], n_past)\n",
    "\n",
    "    lstm = tf.keras.models.Sequential([ \n",
    "        tf.keras.layers.InputLayer(input_shape=[n_past, n_dims]),\n",
    "        tf.keras.layers.BatchNormalization(), \n",
    "\n",
    "        tf.keras.layers.LSTM(neurons[0], return_sequences=True),\n",
    "        tf.keras.layers.Dropout(dropout),\n",
    "\n",
    "        tf.keras.layers.LSTM(neurons[1], return_sequences=True),\n",
    "        tf.keras.layers.Dropout(dropout),\n",
    "\n",
    "        tf.keras.layers.LSTM(neurons[2]),\n",
    "        tf.keras.layers.Dropout(dropout),\n",
    "\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    \n",
    "    def rmse(y_true, y_pred):\n",
    "        loss = backend.sqrt(backend.mean(backend.square((y_true - y_pred))))\n",
    "        return loss\n",
    "\n",
    "    lstm.compile(loss='mse', \n",
    "               optimizer=\"adam\", \n",
    "               metrics=[rmse])\n",
    "    \n",
    "    #checkpoint = ModelCheckpoint('NN_models/lstm_2_layers_with_dropout_nobidirectional.h5',\n",
    "    #                            save_best_only=True,\n",
    "    #                            monitor='val_rmse')\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=100,\n",
    "                                  restore_best_weights=True,\n",
    "                                  monitor='val_rmse')\n",
    "    \n",
    "    lstm_res = lstm.fit(x_train, y_train, \n",
    "                        callbacks= early_stopping,\n",
    "                        validation_split=0.3, shuffle=True,\n",
    "                        verbose=0, batch_size=batch_size, epochs=500)\n",
    "    \n",
    "    x_test, y_test = windowed_dataset(df_test.drop(['Returns', 'RV'], axis=1), df_test['RV'], n_past)\n",
    "    x, returns = windowed_dataset(df_test.drop(['Returns', 'RV'], axis=1), df_test['Returns'], n_past)\n",
    "\n",
    "    # Predictions\n",
    "    pred = lstm.predict(x_test)\n",
    "    df_results = pd.DataFrame({'RV': df_test['RV'].iloc[n_past-1:], 'pred': pred.flatten(), 'returns': returns})\n",
    "\n",
    "    # Calculate the RMSE\n",
    "    rmse = np.sqrt(mse(df_results['RV'], df_results['pred']))\n",
    "\n",
    "    # Calculate the violation ratio\n",
    "    alpha = 0.05\n",
    "    var = pd.DataFrame({'VAR': df_results['pred']* norm.ppf(alpha), 'Returns': df_results['returns']})\n",
    "    violation_ratio = (var['Returns'] < var['VAR']).sum()/len(df_results)\n",
    "\n",
    "    # Calculate the Diebold-Mariano test\n",
    "    df_dmw_test = pd.concat([results_bmk['pred_bmk'], df_results['pred'], df_results['RV']], axis=1).dropna()\n",
    "    dmw_test = dm_test(df_dmw_test['RV'], df_dmw_test['pred_bmk'], df_dmw_test['pred'], one_sided=True)\n",
    "\n",
    "    if plot == True:\n",
    "        plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "        fig, ax = plt.subplots(figsize=(11, 3))\n",
    "        ax.plot(df_results['pred'], label='prediction', color='black', linewidth=0.9)\n",
    "        ax.plot(df_results['RV'], label='RV', color='grey', linewidth=0.7)\n",
    "        ax.set_title('LSTM_3_layers_with_dropout_nobidirectional') \n",
    "        ax.legend(loc='upper left')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.xaxis.set_major_locator(plt.MaxNLocator(6))\n",
    "        plt.show()\n",
    "\n",
    "    return rmse, violation_ratio, dmw_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_with_dropout(df_train, df_test, results_bmk, neurons = [32,16,8], n_past = 22, batch_size = 32, dropout = 0.1, plot = False): \n",
    "\n",
    "    '''\n",
    "    MLP (multi layer perceptron) with dropout. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_train : pandas.DataFrame\n",
    "        Train data.\n",
    "    df_test : pandas.DataFrame\n",
    "        Test data.\n",
    "    results_bmk : pandas.DataFrame\n",
    "        Results of the HAR benchmark model.\n",
    "    neurons : list, optional\n",
    "        Number of neurons in the MLP layers. The default is [32,16,8].\n",
    "    n_past : int, optional\n",
    "        Number of past observations to use as input. The default is 22.\n",
    "    batch_size : int, optional\n",
    "        Batch size. The default is 32.\n",
    "    dropout : float, optional\n",
    "        Dropout rate. The default is 0.1.\n",
    "    plot : bool, optional\n",
    "    ''' \n",
    "\n",
    "    np.random.seed(2024)\n",
    "    n_dims = df_train.shape[1]-2\n",
    "    x_train, y_train = windowed_dataset(df_train.drop(['Returns', 'RV'], axis=1), df_train['RV'], n_past)\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "\n",
    "        tf.keras.layers.InputLayer(input_shape=[n_past, n_dims]),\n",
    "        tf.keras.layers.BatchNormalization(), \n",
    "\n",
    "        tf.keras.layers.Flatten(),\n",
    "\n",
    "        tf.keras.layers.Dense(neurons[0]),\n",
    "        tf.keras.layers.Dropout(dropout),\n",
    "\n",
    "        tf.keras.layers.Dense(neurons[1]),\n",
    "        tf.keras.layers.Dropout(dropout),\n",
    "\n",
    "        tf.keras.layers.Dense(neurons[2]),\n",
    "        tf.keras.layers.Dropout(dropout),\n",
    "\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "\n",
    "    def rmse(y_true, y_pred):\n",
    "        loss = backend.sqrt(backend.mean(backend.square((y_true - y_pred))))\n",
    "        return loss\n",
    "\n",
    "    model.compile(loss='mse', \n",
    "               optimizer=\"adam\", \n",
    "               metrics=[rmse])\n",
    "    \n",
    "    #checkpoint = ModelCheckpoint('NN_models/mlp_with_dropout.h5',\n",
    "    #                            save_best_only=True,\n",
    "    #                            monitor='val_rmse')\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=100,\n",
    "                                  restore_best_weights=True,\n",
    "                                  monitor='val_rmse')\n",
    "    \n",
    "    lstm_res = model.fit(x_train, y_train, \n",
    "                        callbacks= early_stopping,\n",
    "                        validation_split=0.3, shuffle=True,\n",
    "                        verbose=0, batch_size=batch_size, epochs=500)\n",
    "    \n",
    "    x_test, y_test = windowed_dataset(df_test.drop(['Returns', 'RV'], axis=1), df_test['RV'], n_past)\n",
    "    _, returns = windowed_dataset(df_test.drop(['Returns', 'RV'], axis=1), df_test['Returns'], n_past)\n",
    "\n",
    "    # Predictions\n",
    "    pred = model.predict(x_test)\n",
    "    df_results = pd.DataFrame({'RV': df_test['RV'].iloc[n_past-1:], 'pred': pred.flatten(), 'returns': returns})\n",
    "\n",
    "    # Calculate the RMSE\n",
    "    rmse = np.sqrt(mse(df_results['RV'], df_results['pred']))\n",
    "\n",
    "    # Calculate the violation ratio\n",
    "    alpha = 0.05\n",
    "    var = pd.DataFrame({'VAR': df_results['pred']* norm.ppf(alpha), 'Returns': df_results['returns']})\n",
    "    violation_ratio = (var['Returns'] < var['VAR']).sum()/len(df_results)\n",
    "\n",
    "    # Calculate the Diebold-Mariano test\n",
    "    df_dmw_test = pd.concat([results_bmk['pred_bmk'], df_results['pred'], df_results['RV']], axis=1).dropna()\n",
    "    dmw_test = dm_test(df_dmw_test['RV'], df_dmw_test['pred_bmk'], df_dmw_test['pred'], one_sided=True)\n",
    "\n",
    "    if plot == True:\n",
    "        plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "        fig, ax = plt.subplots(figsize=(11, 3))\n",
    "        ax.plot(df_results['pred'], label='prediction', color='black', linewidth=0.9)\n",
    "        ax.plot(df_results['RV'], label='RV', color='grey', linewidth=0.7)\n",
    "        ax.set_title('MLP_with_dropout')\n",
    "        ax.legend(loc='upper left')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.xaxis.set_major_locator(plt.MaxNLocator(6))\n",
    "        plt.show()\n",
    "\n",
    "    return rmse, violation_ratio, dmw_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost(df_train, df_test, results_bmk, n_estimators = 1000, learning_rate=0.01, plot = False):\n",
    "\n",
    "    # Define the model\n",
    "    model = XGBRegressor(n_estimators=n_estimators, learning_rate=learning_rate, n_jobs=10, random_state=2024)\n",
    "\n",
    "    # Fit the model\n",
    "    model.fit(df_train.drop(['Returns', 'RV'], axis=1), df_train['RV'])\n",
    "    prediction = model.predict(df_test.drop(['Returns', 'RV'], axis=1))\n",
    "\n",
    "    # Calculate the RMSE\n",
    "    rmse = np.sqrt(mse(df_test['RV'], prediction))\n",
    "    \n",
    "    results = pd.DataFrame({'pred': prediction, 'RV': df_test['RV']})\n",
    "    results.index = df_test.index\n",
    "\n",
    "\n",
    "    # Calculate the violation ratio\n",
    "    alpha = 0.05\n",
    "    var = pd.DataFrame({'VAR': results['pred']* norm.ppf(alpha), 'Returns': df_test['Returns']})\n",
    "    violation_ratio = (var['Returns'] < var['VAR']).sum()/len(df_test)\n",
    "\n",
    "\n",
    "    # Calculate the Diebold-Mariano test\n",
    "    df_dmw_test = pd.concat([results_bmk['pred_bmk'], results['pred'], df_test['RV']], axis=1)\n",
    "    dmw_test = dm_test(df_dmw_test['RV'], df_dmw_test['pred_bmk'], df_dmw_test['pred'], one_sided=True)\n",
    "\n",
    "    if plot is True:\n",
    "        plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "        fig, ax = plt.subplots(figsize=(11, 3))\n",
    "        ax.plot(results['pred'], label='prediction', color='black', linewidth=0.9)\n",
    "        ax.plot(results['RV'], label='RV', color='grey', linewidth=0.7)\n",
    "        ax.set_title('XGBoost')   \n",
    "        ax.legend(loc='upper left')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.xaxis.set_major_locator(plt.MaxNLocator(6))\n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "    return rmse, violation_ratio, dmw_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_Thesis_Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
